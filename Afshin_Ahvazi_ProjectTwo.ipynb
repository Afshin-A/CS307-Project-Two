{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Five Assignment: Cartpole Problem\n",
    "Review the code in this notebook and in the score_logger.py file in the *scores* folder (directory). Once you have reviewed the code, return to this notebook and select **Cell** and then **Run All** from the menu bar to run this code. The code takes several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random  \n",
    "import gym  \n",
    "import numpy as np  \n",
    "from collections import deque  \n",
    "from keras.models import Sequential  \n",
    "from keras.layers import Dense  \n",
    "from keras.optimizers import Adam  \n",
    "  \n",
    "  \n",
    "from scores.score_logger import ScoreLogger  \n",
    "  \n",
    "ENV_NAME = \"CartPole-v1\"  \n",
    "  \n",
    "GAMMA = 0.95  \n",
    "LEARNING_RATE = 0.001  \n",
    "  \n",
    "MEMORY_SIZE = 1000000  \n",
    "BATCH_SIZE = 20  \n",
    "  \n",
    "EXPLORATION_MAX = 1.0  \n",
    "EXPLORATION_MIN = 0.01  \n",
    "EXPLORATION_DECAY = 0.995  \n",
    "  \n",
    "  \n",
    "class DQNSolver:  \n",
    "  \n",
    "    def __init__(self, observation_space, action_space):  \n",
    "        self.exploration_rate = EXPLORATION_MAX  \n",
    "  \n",
    "        self.action_space = action_space  \n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)  \n",
    "  \n",
    "        self.model = Sequential()  \n",
    "        self.model.add(Dense(24, input_shape=(observation_space,), activation=\"relu\"))  \n",
    "        self.model.add(Dense(24, activation=\"relu\"))  \n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))  \n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))  \n",
    "  \n",
    "    def remember(self, state, action, reward, next_state, done):  \n",
    "        self.memory.append((state, action, reward, next_state, done))  \n",
    "  \n",
    "    def act(self, state):  \n",
    "        if np.random.rand() < self.exploration_rate:  \n",
    "            return random.randrange(self.action_space)  \n",
    "        q_values = self.model.predict(state)  \n",
    "        return np.argmax(q_values[0])  \n",
    "  \n",
    "    def experience_replay(self):  \n",
    "        if len(self.memory) < BATCH_SIZE:  \n",
    "            return  \n",
    "        batch = random.sample(self.memory, BATCH_SIZE)  \n",
    "        for state, action, reward, state_next, terminal in batch:  \n",
    "            q_update = reward  \n",
    "            if not terminal:  \n",
    "                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))  \n",
    "            q_values = self.model.predict(state)  \n",
    "            q_values[0][action] = q_update  \n",
    "            self.model.fit(state, q_values, verbose=0)  \n",
    "        self.exploration_rate *= EXPLORATION_DECAY  \n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)  \n",
    "  \n",
    "  \n",
    "def cartpole():  \n",
    "    env = gym.make(ENV_NAME)  \n",
    "    score_logger = ScoreLogger(ENV_NAME)  \n",
    "    observation_space = env.observation_space.shape[0]  \n",
    "    action_space = env.action_space.n  \n",
    "    dqn_solver = DQNSolver(observation_space, action_space)  \n",
    "    run = 0  \n",
    "    while True:  \n",
    "        run += 1  \n",
    "        state = env.reset()  \n",
    "        state = np.reshape(state, [1, observation_space])  \n",
    "        step = 0  \n",
    "        while True:  \n",
    "            step += 1  \n",
    "            #env.render()  \n",
    "            action = dqn_solver.act(state)  \n",
    "            state_next, reward, terminal, info = env.step(action)  \n",
    "            reward = reward if not terminal else -reward  \n",
    "            state_next = np.reshape(state_next, [1, observation_space])  \n",
    "            dqn_solver.remember(state, action, reward, state_next, terminal)  \n",
    "            state = state_next  \n",
    "            if terminal:  \n",
    "                print (\"Run: \" + str(run) + \", exploration: \" + str(dqn_solver.exploration_rate) + \", score: \" + str(step))  \n",
    "                score_logger.add_score(step, run)  \n",
    "                break  \n",
    "            dqn_solver.experience_replay()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1, exploration: 0.8690529955452602, score: 48\n",
      "Scores: (min: 48, avg: 48, max: 48)\n",
      "\n",
      "Run: 2, exploration: 0.8224322824348486, score: 12\n",
      "Scores: (min: 12, avg: 30, max: 48)\n",
      "\n",
      "Run: 3, exploration: 0.7822236754458713, score: 11\n",
      "Scores: (min: 11, avg: 23.666666666666668, max: 48)\n",
      "\n",
      "Run: 4, exploration: 0.7328768546436799, score: 14\n",
      "Scores: (min: 11, avg: 21.25, max: 48)\n",
      "\n",
      "Run: 5, exploration: 0.6696478204705644, score: 19\n",
      "Scores: (min: 11, avg: 20.8, max: 48)\n",
      "\n",
      "Run: 6, exploration: 0.567555222460375, score: 34\n",
      "Scores: (min: 11, avg: 23, max: 48)\n",
      "\n",
      "Run: 7, exploration: 0.5211953074858876, score: 18\n",
      "Scores: (min: 11, avg: 22.285714285714285, max: 48)\n",
      "\n",
      "Run: 8, exploration: 0.4982051627146237, score: 10\n",
      "Scores: (min: 10, avg: 20.75, max: 48)\n",
      "\n",
      "Run: 9, exploration: 0.47862223409330756, score: 9\n",
      "Scores: (min: 9, avg: 19.444444444444443, max: 48)\n",
      "\n",
      "Run: 10, exploration: 0.4417353564707963, score: 17\n",
      "Scores: (min: 9, avg: 19.2, max: 48)\n",
      "\n",
      "Run: 11, exploration: 0.4222502236424958, score: 10\n",
      "Scores: (min: 9, avg: 18.363636363636363, max: 48)\n",
      "\n",
      "Run: 12, exploration: 0.39166620452737816, score: 16\n",
      "Scores: (min: 9, avg: 18.166666666666668, max: 48)\n",
      "\n",
      "Run: 13, exploration: 0.37251769488706843, score: 11\n",
      "Scores: (min: 9, avg: 17.615384615384617, max: 48)\n",
      "\n",
      "Run: 14, exploration: 0.3173112652388396, score: 33\n",
      "Scores: (min: 9, avg: 18.714285714285715, max: 48)\n",
      "\n",
      "Run: 15, exploration: 0.3063705780533402, score: 8\n",
      "Scores: (min: 8, avg: 18, max: 48)\n",
      "\n",
      "Run: 16, exploration: 0.2913921604631864, score: 11\n",
      "Scores: (min: 8, avg: 17.5625, max: 48)\n",
      "\n",
      "Run: 17, exploration: 0.26227842021373715, score: 22\n",
      "Scores: (min: 8, avg: 17.823529411764707, max: 48)\n",
      "\n",
      "Run: 18, exploration: 0.23371867538818816, score: 24\n",
      "Scores: (min: 8, avg: 18.166666666666668, max: 48)\n",
      "\n",
      "Run: 19, exploration: 0.2041345879004775, score: 28\n",
      "Scores: (min: 8, avg: 18.68421052631579, max: 48)\n",
      "\n",
      "Run: 20, exploration: 0.16048131420416054, score: 49\n",
      "Scores: (min: 8, avg: 20.2, max: 49)\n",
      "\n",
      "Run: 21, exploration: 0.12743425563174798, score: 47\n",
      "Scores: (min: 8, avg: 21.476190476190474, max: 49)\n",
      "\n",
      "Run: 22, exploration: 0.09480864735409487, score: 60\n",
      "Scores: (min: 8, avg: 23.227272727272727, max: 60)\n",
      "\n",
      "Run: 23, exploration: 0.0712465030521374, score: 58\n",
      "Scores: (min: 8, avg: 24.73913043478261, max: 60)\n",
      "\n",
      "Run: 24, exploration: 0.05354009722551939, score: 58\n",
      "Scores: (min: 8, avg: 26.125, max: 60)\n",
      "\n",
      "Run: 25, exploration: 0.031948941366546975, score: 104\n",
      "Scores: (min: 8, avg: 29.24, max: 104)\n",
      "\n",
      "Run: 26, exploration: 0.029933432588273214, score: 14\n",
      "Scores: (min: 8, avg: 28.653846153846153, max: 104)\n",
      "\n",
      "Run: 27, exploration: 0.021610099696926857, score: 66\n",
      "Scores: (min: 8, avg: 30.037037037037038, max: 104)\n",
      "\n",
      "Run: 28, exploration: 0.016239489508417658, score: 58\n",
      "Scores: (min: 8, avg: 31.035714285714285, max: 104)\n",
      "\n",
      "Run: 29, exploration: 0.01071243191960775, score: 84\n",
      "Scores: (min: 8, avg: 32.86206896551724, max: 104)\n",
      "\n",
      "Run: 30, exploration: 0.01, score: 69\n",
      "Scores: (min: 8, avg: 34.06666666666667, max: 104)\n",
      "\n",
      "Run: 31, exploration: 0.01, score: 109\n",
      "Scores: (min: 8, avg: 36.483870967741936, max: 109)\n",
      "\n",
      "Run: 32, exploration: 0.01, score: 68\n",
      "Scores: (min: 8, avg: 37.46875, max: 109)\n",
      "\n",
      "Run: 33, exploration: 0.01, score: 78\n",
      "Scores: (min: 8, avg: 38.696969696969695, max: 109)\n",
      "\n",
      "Run: 34, exploration: 0.01, score: 137\n",
      "Scores: (min: 8, avg: 41.588235294117645, max: 137)\n",
      "\n",
      "Run: 35, exploration: 0.01, score: 86\n",
      "Scores: (min: 8, avg: 42.857142857142854, max: 137)\n",
      "\n",
      "Run: 36, exploration: 0.01, score: 171\n",
      "Scores: (min: 8, avg: 46.416666666666664, max: 171)\n",
      "\n",
      "Run: 37, exploration: 0.01, score: 280\n",
      "Scores: (min: 8, avg: 52.729729729729726, max: 280)\n",
      "\n",
      "Run: 38, exploration: 0.01, score: 314\n",
      "Scores: (min: 8, avg: 59.60526315789474, max: 314)\n",
      "\n",
      "Run: 39, exploration: 0.01, score: 500\n",
      "Scores: (min: 8, avg: 70.8974358974359, max: 500)\n",
      "\n",
      "Run: 40, exploration: 0.01, score: 500\n",
      "Scores: (min: 8, avg: 81.625, max: 500)\n",
      "\n",
      "Run: 41, exploration: 0.01, score: 500\n",
      "Scores: (min: 8, avg: 91.82926829268293, max: 500)\n",
      "\n",
      "Run: 42, exploration: 0.01, score: 500\n",
      "Scores: (min: 8, avg: 101.54761904761905, max: 500)\n",
      "\n",
      "Run: 43, exploration: 0.01, score: 500\n",
      "Scores: (min: 8, avg: 110.81395348837209, max: 500)\n",
      "\n",
      "Run: 44, exploration: 0.01, score: 500\n",
      "Scores: (min: 8, avg: 119.6590909090909, max: 500)\n",
      "\n",
      "Run: 45, exploration: 0.01, score: 500\n",
      "Scores: (min: 8, avg: 128.11111111111111, max: 500)\n",
      "\n",
      "Run: 46, exploration: 0.01, score: 500\n",
      "Scores: (min: 8, avg: 136.19565217391303, max: 500)\n",
      "\n",
      "Run: 47, exploration: 0.01, score: 462\n",
      "Scores: (min: 8, avg: 143.12765957446808, max: 500)\n",
      "\n",
      "Run: 48, exploration: 0.01, score: 500\n",
      "Scores: (min: 8, avg: 150.5625, max: 500)\n",
      "\n",
      "Run: 49, exploration: 0.01, score: 324\n",
      "Scores: (min: 8, avg: 154.10204081632654, max: 500)\n",
      "\n",
      "Run: 50, exploration: 0.01, score: 285\n",
      "Scores: (min: 8, avg: 156.72, max: 500)\n",
      "\n",
      "Run: 51, exploration: 0.01, score: 262\n",
      "Scores: (min: 8, avg: 158.7843137254902, max: 500)\n",
      "\n",
      "Run: 52, exploration: 0.01, score: 8\n",
      "Scores: (min: 8, avg: 155.8846153846154, max: 500)\n",
      "\n",
      "Run: 53, exploration: 0.01, score: 11\n",
      "Scores: (min: 8, avg: 153.1509433962264, max: 500)\n",
      "\n",
      "Run: 54, exploration: 0.01, score: 398\n",
      "Scores: (min: 8, avg: 157.6851851851852, max: 500)\n",
      "\n",
      "Run: 55, exploration: 0.01, score: 327\n",
      "Scores: (min: 8, avg: 160.76363636363635, max: 500)\n",
      "\n",
      "Run: 56, exploration: 0.01, score: 434\n",
      "Scores: (min: 8, avg: 165.64285714285714, max: 500)\n",
      "\n",
      "Run: 57, exploration: 0.01, score: 350\n",
      "Scores: (min: 8, avg: 168.87719298245614, max: 500)\n",
      "\n",
      "Run: 58, exploration: 0.01, score: 231\n",
      "Scores: (min: 8, avg: 169.94827586206895, max: 500)\n",
      "\n",
      "Run: 59, exploration: 0.01, score: 373\n",
      "Scores: (min: 8, avg: 173.38983050847457, max: 500)\n",
      "\n",
      "Run: 60, exploration: 0.01, score: 330\n",
      "Scores: (min: 8, avg: 176, max: 500)\n",
      "\n",
      "Run: 61, exploration: 0.01, score: 447\n",
      "Scores: (min: 8, avg: 180.44262295081967, max: 500)\n",
      "\n",
      "Run: 62, exploration: 0.01, score: 500\n",
      "Scores: (min: 8, avg: 185.59677419354838, max: 500)\n",
      "\n",
      "Run: 63, exploration: 0.01, score: 271\n",
      "Scores: (min: 8, avg: 186.95238095238096, max: 500)\n",
      "\n",
      "Run: 64, exploration: 0.01, score: 348\n",
      "Scores: (min: 8, avg: 189.46875, max: 500)\n",
      "\n",
      "Run: 65, exploration: 0.01, score: 223\n",
      "Scores: (min: 8, avg: 189.98461538461538, max: 500)\n",
      "\n",
      "Run: 66, exploration: 0.01, score: 281\n",
      "Scores: (min: 8, avg: 191.36363636363637, max: 500)\n",
      "\n",
      "Run: 67, exploration: 0.01, score: 412\n",
      "Scores: (min: 8, avg: 194.65671641791045, max: 500)\n",
      "\n",
      "Run: 68, exploration: 0.01, score: 261\n",
      "Scores: (min: 8, avg: 195.63235294117646, max: 500)\n",
      "\n",
      "Run: 69, exploration: 0.01, score: 349\n",
      "Scores: (min: 8, avg: 197.85507246376812, max: 500)\n",
      "\n",
      "Run: 70, exploration: 0.01, score: 252\n",
      "Scores: (min: 8, avg: 198.62857142857143, max: 500)\n",
      "\n",
      "Run: 71, exploration: 0.01, score: 237\n",
      "Scores: (min: 8, avg: 199.16901408450704, max: 500)\n",
      "\n",
      "Run: 72, exploration: 0.01, score: 239\n",
      "Scores: (min: 8, avg: 199.72222222222223, max: 500)\n",
      "\n",
      "Run: 73, exploration: 0.01, score: 458\n",
      "Scores: (min: 8, avg: 203.26027397260273, max: 500)\n",
      "\n",
      "Run: 74, exploration: 0.01, score: 225\n",
      "Scores: (min: 8, avg: 203.55405405405406, max: 500)\n",
      "\n",
      "Run: 75, exploration: 0.01, score: 451\n",
      "Scores: (min: 8, avg: 206.85333333333332, max: 500)\n",
      "\n",
      "Run: 76, exploration: 0.01, score: 500\n",
      "Scores: (min: 8, avg: 210.71052631578948, max: 500)\n",
      "\n",
      "Run: 77, exploration: 0.01, score: 417\n",
      "Scores: (min: 8, avg: 213.3896103896104, max: 500)\n",
      "\n",
      "Run: 78, exploration: 0.01, score: 343\n",
      "Scores: (min: 8, avg: 215.05128205128204, max: 500)\n",
      "\n",
      "Run: 79, exploration: 0.01, score: 267\n",
      "Scores: (min: 8, avg: 215.70886075949366, max: 500)\n",
      "\n",
      "Run: 80, exploration: 0.01, score: 313\n",
      "Scores: (min: 8, avg: 216.925, max: 500)\n",
      "\n",
      "Run: 81, exploration: 0.01, score: 331\n",
      "Scores: (min: 8, avg: 218.33333333333334, max: 500)\n",
      "\n",
      "Run: 82, exploration: 0.01, score: 395\n",
      "Scores: (min: 8, avg: 220.4878048780488, max: 500)\n",
      "\n",
      "Run: 83, exploration: 0.01, score: 307\n",
      "Scores: (min: 8, avg: 221.53012048192772, max: 500)\n",
      "\n",
      "Run: 84, exploration: 0.01, score: 298\n",
      "Scores: (min: 8, avg: 222.4404761904762, max: 500)\n",
      "\n",
      "Run: 85, exploration: 0.01, score: 500\n",
      "Scores: (min: 8, avg: 225.7058823529412, max: 500)\n",
      "\n",
      "Run: 86, exploration: 0.01, score: 178\n",
      "Scores: (min: 8, avg: 225.15116279069767, max: 500)\n",
      "\n",
      "Run: 87, exploration: 0.01, score: 500\n",
      "Scores: (min: 8, avg: 228.31034482758622, max: 500)\n",
      "\n",
      "Run: 88, exploration: 0.01, score: 314\n",
      "Scores: (min: 8, avg: 229.2840909090909, max: 500)\n",
      "\n",
      "Run: 89, exploration: 0.01, score: 241\n",
      "Scores: (min: 8, avg: 229.41573033707866, max: 500)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 90, exploration: 0.01, score: 179\n",
      "Scores: (min: 8, avg: 228.85555555555555, max: 500)\n",
      "\n",
      "Run: 91, exploration: 0.01, score: 279\n",
      "Scores: (min: 8, avg: 229.4065934065934, max: 500)\n",
      "\n",
      "Run: 92, exploration: 0.01, score: 322\n",
      "Scores: (min: 8, avg: 230.41304347826087, max: 500)\n",
      "\n",
      "Run: 93, exploration: 0.01, score: 127\n",
      "Scores: (min: 8, avg: 229.30107526881721, max: 500)\n",
      "\n",
      "Run: 94, exploration: 0.01, score: 500\n",
      "Scores: (min: 8, avg: 232.18085106382978, max: 500)\n",
      "\n",
      "Run: 95, exploration: 0.01, score: 167\n",
      "Scores: (min: 8, avg: 231.49473684210525, max: 500)\n",
      "\n",
      "Run: 96, exploration: 0.01, score: 361\n",
      "Scores: (min: 8, avg: 232.84375, max: 500)\n",
      "\n",
      "Run: 97, exploration: 0.01, score: 437\n",
      "Scores: (min: 8, avg: 234.94845360824743, max: 500)\n",
      "\n",
      "Run: 98, exploration: 0.01, score: 225\n",
      "Scores: (min: 8, avg: 234.8469387755102, max: 500)\n",
      "\n",
      "Run: 99, exploration: 0.01, score: 192\n",
      "Scores: (min: 8, avg: 234.41414141414143, max: 500)\n",
      "\n",
      "Run: 100, exploration: 0.01, score: 51\n",
      "Scores: (min: 8, avg: 232.58, max: 500)\n",
      "\n",
      "Solved in 0 runs, 100 total runs.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e23913c48dc8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcartpole\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-bea6327a4fd8>\u001b[0m in \u001b[0;36mcartpole\u001b[1;34m()\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mterminal\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Run: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\", exploration: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdqn_solver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexploration_rate\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\", score: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                 \u001b[0mscore_logger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[0mdqn_solver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mU:\\scores\\score_logger.py\u001b[0m in \u001b[0;36madd_score\u001b[1;34m(self, score, run)\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[1;31m#               show_trend=False,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;31m#               show_legend=False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m             \u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_save_png\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage_of_n_last\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_goal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_trend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_legend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "cartpole()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If the code is running properly, you should begin to see output appearing above this code block. It will take several minutes, so it is recommended that you let this code run in the background while completing other work. When the code has finished, it will print output saying, \"Solved in _ runs, _ total runs.\"\n",
    "\n",
    "You may see an error about not having an exit command. This error does not affect the program's functionality and results from the steps taken to convert the code from Python 2.x to Python 3. Please disregard this error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random  \n",
    "import gym  \n",
    "import numpy as np  \n",
    "from collections import deque  \n",
    "from keras.models import Sequential  \n",
    "from keras.layers import Dense  \n",
    "from keras.optimizers import Adam  \n",
    "  \n",
    "  \n",
    "from scores.score_logger import ScoreLogger  \n",
    "  \n",
    "ENV_NAME = \"CartPole-v1\"  \n",
    "  \n",
    "GAMMA = 0.95  \n",
    "LEARNING_RATE = 0.01  \n",
    "  \n",
    "MEMORY_SIZE = 1000000  \n",
    "BATCH_SIZE = 20  \n",
    "  \n",
    "EXPLORATION_MAX = 1.0  \n",
    "EXPLORATION_MIN = 0.01  \n",
    "EXPLORATION_DECAY = 0.999  \n",
    "  \n",
    "  \n",
    "class DQNSolver:  \n",
    "  \n",
    "    def __init__(self, observation_space, action_space):  \n",
    "        self.exploration_rate = EXPLORATION_MAX  \n",
    "  \n",
    "        self.action_space = action_space  \n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)  \n",
    "  \n",
    "        self.model = Sequential()  \n",
    "        self.model.add(Dense(24, input_shape=(observation_space,), activation=\"relu\"))  \n",
    "        self.model.add(Dense(24, activation=\"relu\"))  \n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))  \n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))  \n",
    "  \n",
    "    def remember(self, state, action, reward, next_state, done):  \n",
    "        self.memory.append((state, action, reward, next_state, done))  \n",
    "  \n",
    "    def act(self, state):  \n",
    "        if np.random.rand() < self.exploration_rate:  \n",
    "            return random.randrange(self.action_space)  \n",
    "        q_values = self.model.predict(state)  \n",
    "        return np.argmax(q_values[0])  \n",
    "  \n",
    "    def experience_replay(self):  \n",
    "        if len(self.memory) < BATCH_SIZE:  \n",
    "            return  \n",
    "        batch = random.sample(self.memory, BATCH_SIZE)  \n",
    "        for state, action, reward, state_next, terminal in batch:  \n",
    "            q_update = reward  \n",
    "            if not terminal:  \n",
    "                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))  \n",
    "            q_values = self.model.predict(state)  \n",
    "            q_values[0][action] = q_update  \n",
    "            self.model.fit(state, q_values, verbose=0)  \n",
    "        self.exploration_rate *= EXPLORATION_DECAY  \n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)  \n",
    "  \n",
    "  \n",
    "def cartpole():  \n",
    "    env = gym.make(ENV_NAME)  \n",
    "    score_logger = ScoreLogger(ENV_NAME)  \n",
    "    observation_space = env.observation_space.shape[0]  \n",
    "    action_space = env.action_space.n  \n",
    "    dqn_solver = DQNSolver(observation_space, action_space)  \n",
    "    run = 0  \n",
    "    while True:  \n",
    "        run += 1  \n",
    "        state = env.reset()  \n",
    "        state = np.reshape(state, [1, observation_space])  \n",
    "        step = 0  \n",
    "        while True:  \n",
    "            step += 1  \n",
    "            #env.render()  \n",
    "            action = dqn_solver.act(state)  \n",
    "            state_next, reward, terminal, info = env.step(action)  \n",
    "            reward = reward if not terminal else -reward  \n",
    "            state_next = np.reshape(state_next, [1, observation_space])  \n",
    "            dqn_solver.remember(state, action, reward, state_next, terminal)  \n",
    "            state = state_next  \n",
    "            if terminal:  \n",
    "                print (\"Run: \" + str(run) + \", exploration: \" + str(dqn_solver.exploration_rate) + \", score: \" + str(step))  \n",
    "                score_logger.add_score(step, run)  \n",
    "                break  \n",
    "            dqn_solver.experience_replay()  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
